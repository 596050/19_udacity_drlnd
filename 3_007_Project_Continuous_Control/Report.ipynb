{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project\n",
    "## Description\n",
    "\n",
    "To solve the **Reacher** challenge, we have employed a _reinforcement learning_ algorithm named **\"DDPG\"**, which stands for _\"Distributed Distributional Deterministic Policy Gradients\"_, explained [here](https://openreview.net/pdf?id=SyZipzbCb).\n",
    "\n",
    "The core of this algorithm is resumed in:\n",
    " - There are two kind of networks\n",
    "     - **Actor**: Network that predicts the actions, given a state; in a continupus manner.\n",
    "     - **Critic**: given the information about the environment and action selected, estimates the reward.\n",
    " - The algorithm is suited for contiuous action and state spaces.\n",
    " - In order to have a balance between **\"exploration/explotation\"**, a noise decaying coefficient is added to the action performed by the agent, i.e. after the _\"Actor\"_ has predicted and action, the noise is added.\n",
    " - Both __actor__ and __critic__ count with their respective _target networks_\n",
    " - Update of networks is produced by __expericence replay__\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "### Networks\n",
    "#### Actor\n",
    " The actor model is based on a neural network, with the following architecture:\n",
    " - __Input__: 33 inputs, state size\n",
    " - __1st Layer__: 32 units | Layer Normalization | Relu Activation\n",
    " - __2nd Layer__: 32 units | Layer Normalization | Relu Activation\n",
    " - __Output__: 4 units (action size) | Layer Normalization | Relu Activation | Hyperbolic Tangent\n",
    "\n",
    "#### Critic\n",
    "The critic, had a much complex architecture, has _between the 1st and 2nd layers_ we __added the action's values from the Actor network__.\n",
    " - __Input__: 33 inputs, state size\n",
    " - __1st Layer__: 256 units + 4 units (actor output) | Layer Normalization | Relu Activation\n",
    " - __2nd Layer__: 256 units | Layer Normalization | Relu Activation\n",
    " - __3rd Layer__: 128 units | Layer Normalization | Relu Activation\n",
    " - __4rth Layer__: 64 units | Layer Normalization | Relu Activation\n",
    " - __Output__: 1 unit (Q value) + Layer Normalization + Relu Activation + Hyperbolic Tangent\n",
    " \n",
    "\n",
    "### Hyperparameters\n",
    " - __Batch Size__ = 128\n",
    " - __Gamma__ = 0.99\n",
    " - __Tau__ = 1e-3\n",
    " - __LR Actor__ = 1e-4\n",
    " - __LR Critic__ = 2e-4\n",
    " - __Weight Decay__ = 1e-4\n",
    " - __Exploration Coef.__ = 1.0 (initial value)\n",
    " - __Exploration Coef. decay__ = 0.95 (each time we learnt we multiplied the _\"exploratory coef.\"_ by it)\n",
    " - __Update Every__ = 10 (number of iterations between learnings)\n",
    " - __Consecutive Learning Iterations__ = 4 (number of learning iterations in a row)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "### Raw results\n",
    "Results obtained from last algorithm training iterations.\n",
    "\n",
    "![python_console](ddpg_results.png)\n",
    "\n",
    "\n",
    "### Plot results\n",
    "Plot of the total historical learning of the agent-critic.\n",
    "![resutls_plot](ddpg_plot.png)\n",
    "\n",
    "The results were of a mean (windows of 100 values) of __above 30.0 points__ of reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Improvements\n",
    "Improvements that ought to increase the learning speed and stability. \n",
    " - **Prioritized Experience Replay**: being able to weigth the best experiences (i.e. the most recent,, the ones that gave mor reward).\n",
    " - **Implementing other algorithtms**: as __Q-Prop__ [algorithm](https://arxiv.org/abs/1611.02247v1), which improves the main problem of policy grandient methods (e.g _\"DDPG\"_), that is they required big hyperparameter optimization to find a convergent and stable point. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
