{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0| Project Overlook\n",
    "\n",
    "The original goal of the project was to use the __DQN algorithm__ descibed in the [\"paper\"](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf). Additional feature inclusion were recommended.\n",
    "\n",
    "\n",
    "# 1| Initial Implementation\n",
    "The initial implementation consisted in porting the __DQN algorithm__ already worked in a previous chapter to the concrete case of the _Banana's environement_. The out of the box hyper-parameters worked already satisfactory on the given challenge; obtaining the required score of __+13__ by the episode between 400-500 (well before the 1000 required). \n",
    "\n",
    "__Neural Network Architecture:__ \n",
    "- Fully connected layer: {input: 37 , output: 64}\n",
    "- Fully connected layer: {input: 64, output:64}\n",
    "- Fully connected layer: {input: 64, output: 4}\n",
    "\n",
    "__Hyper-parameters:__\n",
    "- Epsilon start = 1.0\n",
    "- Epsilon end = 0.01\n",
    "- Epsilon decay = 0.995\n",
    "- Number episodes = 2000\n",
    "\n",
    "Other configurations with additional layers and/or neurons where tested, on the same DQN algorithm, obtainind worse results; indicating that the __current architecture__ of the network is properly suited for the challenge.\n",
    "\n",
    "### Results:\n",
    "![title](plots/QNetworks_v1_results.jpeg)\n",
    "\n",
    "\n",
    "# 2| Second implementation\n",
    "The second implementation, __not on `Navigation.ipynb`__ (present in the files with naming `_002_`), tried to implement the _Prioritized Experience Replay_ as described on this [paper](https://arxiv.org/abs/1511.05952). __Unfortunately__ after investing many hours, the code is functional, though __unable to learn__. \n",
    "\n",
    "### Results:\n",
    "![title](plots/QNetworks_PER_results.jpeg)\n",
    "\n",
    "__Possible causes:__\n",
    "- Bad implementation of the _priority update code_\n",
    "- Unproper implementation of the _temporal difference weighted error_, due to the lack of full understanding on Pytorch's library.\n",
    "\n",
    "\n",
    "# 3| Future implementations\n",
    "Aditional improvements:\n",
    "- _Algorithm_ : \n",
    "    - __Dueling Q-Network__, for an improvement of the modelization of true Q(s,a) function. By decoupling the inherent value of a given \"state\" form the available actions, as described in [paper](https://arxiv.org/abs/1511.06581)\n",
    "    - __Rainbow algorithm__, combining strengths of several deep learning algorithms to outperform current DQN implementations, as described on [paper](https://arxiv.org/pdf/1710.02298.pdf) \n",
    "    \n",
    "- _Usability_:\n",
    "    - Implement the usage of the _\"computer vision\"_ version of the environement, in order to practise with the imput a human would receive. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
