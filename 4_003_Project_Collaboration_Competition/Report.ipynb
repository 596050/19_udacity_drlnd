{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 - Collaboration Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "The project consists in training a learning agent in order to play the a tennis game. For each __properly hit the ball over the net__ the agent receives a reward of +0.1, __if the ball hit the ground or hits the ball out of bound__, the agent is gaven a -0.1.\n",
    "\n",
    "![image](plots/tennis_environment.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Algorithm\n",
    "\n",
    "The approach used for the solution of this challenge is base in the implementation of the [DDPG algorithm](https://arxiv.org/pdf/1509.02971.pdf).\n",
    "\n",
    "__The algorithm is__:\n",
    "- It's based on a dual network architecture, based on:\n",
    "  - An __\"Actor\"__: charged of modelizing the actions on the multidimentional space, by having as input the observed state of the environment of each agent.\n",
    "  - A __\"Critic\"__: charged of modelizing the Q-table by using the temporal difference method. Has as inputs the observations and on and posterior level the actions of all agents, thus being able to produce a better estimate of the \"Q\" value of the current state.\n",
    "\n",
    "- The Critic is not used while testing, so only the partial information observed by each agent is employed; the motivation is due to __sabilizing the learning__.\n",
    "    \n",
    "- The trade-off between __exploration/explotation__ is controled by a coefficient of exploration, that is realted to the __noise__. Reducing it's value with the progession of learning. \n",
    "\n",
    "### The algorithm process:\n",
    "![algorithm](plots/ddpg_algorithm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "The implementation has been tweaked in many different ways, changing many hyper-parameters, the way the network learns (after each simulator episode or after a batch of them), modifying the _ReplayBuffer_ size in order to keep __more recent__ or __older__ experiences for relearning (additionally trying to split into two different buffers, one for agent). The final satisfactory _model parameters_ and _hyperparameters_ are: \n",
    "\n",
    "__The Hyper-parameters:__\n",
    " - Buffer Size: 1e5   \n",
    " - Batch Size: 512          \n",
    " - Gamma: 0.99             \n",
    " - Tau: 2e-3             \n",
    " - Actor Learning Rate: 1e-4 \n",
    " - Critic Learning Rate:  3e-4  \n",
    " - Weight Decay: 0.0  \n",
    " - Update Every: 5\n",
    " - Consecutive Learning: 10 \n",
    " \n",
    "__Model Architecture:__\n",
    " - Actor:\n",
    "   - Layer 1: 512 units (relu)\n",
    "   - Layer 2: 256 units (relu)\n",
    "   - Layer 3: 2 units (tanh)\n",
    "\n",
    " - Critic:\n",
    "   - Layer 1: 512 units (relu)\n",
    "   - Layer 2: 256 units (relu)\n",
    "   - Layer 3: 2 units (tanh)\n",
    " \n",
    "__Files:__\n",
    "   - Tennis.py \n",
    "   - Tennis.ipynb\n",
    "   - ddpg.py\n",
    "   - model.py\n",
    "   \n",
    "### Results:\n",
    "Plot of the attained results after aproximatyvely 300 iterations\n",
    "![plot_results](plots/results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resume\n",
    "The **resolution** of this project has been **extremely long** as `poor election of hyper-parameters` has caused a huge delay in the satosfactory obtention of results; as other brought by other [researchers](https://arxiv.org/pdf/1611.02247v1.pdf), this method is __extremely unstable__ and may require from __long and exhaustive hyper-parameter search__; _which has been the case_.\n",
    "\n",
    "### Future ideas to implment\n",
    "Try new an diverse algorithms, as:\n",
    "- __MADDPG__: multiagent environement; [MADDPG](https://arxiv.org/pdf/1706.02275.pdf) (Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments). Though this brings other problems as own to properly share the information to the agents so the are always fully aware of the total state, _to garantee convergence_.\n",
    "- __GAIL__: Generative Adversarial Imitation Learning; model-free imitation learning algorithm that obtains significant performance gains over existing model-free methods in imitating complex behaviors in large, high-dimensional environments. As described in the following [paper](https://arxiv.org/abs/1606.03476). \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
